{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_train_orig = pd.read_csv(\"C:\\\\Users\\\\FAKHRE\\\\Music\\\\Data\\\\market.csv\")\n",
    "news_train_orig = pd.read_csv(\"C:\\\\Users\\\\FAKHRE\\\\Music\\\\Data\\\\news.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Copy of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market train shape:  (15000, 16)\n",
      "News train shape:  (30000, 35)\n"
     ]
    }
   ],
   "source": [
    "market_train_df = market_train_orig.copy()\n",
    "news_train_df = news_train_orig.copy()\n",
    "print('Market train shape: ',market_train_df.shape)\n",
    "print('News train shape: ', news_train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting time variable into year, month, date, hour and in minutes formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#market_train_df['date'] =  pd.to_datetime(market_train_df['time'], format='%Y%m%d:%H:%M:%S.%f')\n",
    "market_train_df['time'] = pd.to_datetime(market_train_df['time'], format='%Y-%m-%d %H:%M')\n",
    "news_train_df['time'] = pd.to_datetime(news_train_df['time'], format='%Y-%m-%d %H:%M')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Market data\n",
    "\n",
    "### Preprocessing of Market Data\n",
    "\n",
    "##### Fill nulls - Market values: All null data comes from market adjusted columns. We fill them up with the raw values in the same row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seprating values in two part column market and column raw\n",
    "##Market values: All null data comes from market adjusted columns. We fill them up with the raw values in the same row\n",
    "column_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n",
    "column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n",
    "\n",
    "for i in range(len(column_raw)):\n",
    "    market_train_df[column_market[i]] = market_train_df[column_market[i]].fillna(market_train_df[column_raw[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers-Returns: Return should not exceed 50% or falls below 50%. If it does, it is either noise, or extreme data that will confuse our prediction later on. We remove these extreme data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing outliers ...\n",
      "There were 4 lines removed\n"
     ]
    }
   ],
   "source": [
    "### Removing outlier\n",
    "print('Removing outliers ...')\n",
    "column_return = column_market + column_raw + ['returnsOpenNextMktres10']\n",
    "orig_len = market_train_df.shape[0]\n",
    "for column in column_return:\n",
    "    market_train_df = market_train_df.loc[market_train_df[column]>=-2]\n",
    "    market_train_df = market_train_df.loc[market_train_df[column]<=2]\n",
    "new_len = market_train_df.shape[0]\n",
    "rmv_len = np.abs(orig_len-new_len)\n",
    "print('There were %i lines removed' %rmv_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove strange data: Here we remove data with unknown asset name or asset codes with strange behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing strange data ...\n",
      "There were 0 lines removed\n"
     ]
    }
   ],
   "source": [
    "#### Removing Starnge data\n",
    "print('Removing strange data ...')\n",
    "orig_len = market_train_df.shape[0]\n",
    "market_train_df = market_train_df[~market_train_df['assetCode'].isin(['PGN.N','EBRYY.OB'])]\n",
    "#market_train_df = market_train_df[~market_train_df['assetName'].isin(['Unknown'])]\n",
    "new_len = market_train_df.shape[0]\n",
    "rmv_len = np.abs(orig_len-new_len)\n",
    "print('There were %i lines removed' %rmv_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News data\n",
    "\n",
    "Remove outliers: apply a clip filter to reduce too extreme data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers\n",
    "def remove_outliers(data_frame, column_list, low=0.02, high=0.98):\n",
    "    for column in column_list:\n",
    "        this_column = data_frame[column]\n",
    "        quant_df = this_column.quantile([low,high])\n",
    "        low_limit = quant_df[low]\n",
    "        high_limit = quant_df[high]\n",
    "        data_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping news outliers ...\n"
     ]
    }
   ],
   "source": [
    "# Remove outlier\n",
    "columns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence','noveltyCount12H',\\\n",
    "                  'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H',\\\n",
    "                  'volumeCounts3D','volumeCounts5D','volumeCounts7D']\n",
    "print('Clipping news outliers ...')\n",
    "news_train_df = remove_outliers(news_train_df, columns_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering\n",
    "#### Data processing function\n",
    "Here we make a function process both market and news data, then merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_code_dict = {k: v for v, k in enumerate(market_train_df['assetCode'].unique())}\n",
    "drop_columns = [col for col in news_train_df.columns if col not in ['sourceTimestamp', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n",
    "               'sentenceCount', 'firstMentionSentence', 'relevance','firstCreated', 'assetCodes']]\n",
    "columns_news = ['firstCreated','relevance','sentimentClass','sentimentNegative','sentimentNeutral',\n",
    "               'sentimentPositive','noveltyCount24H','noveltyCount7D','volumeCounts24H','volumeCounts7D','assetCodes','sourceTimestamp',\n",
    "               'assetName','audiences', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n",
    "               'sentenceCount', 'firstMentionSentence','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing function\n",
    "def data_prep(market_df,news_df):\n",
    "    market_df['date'] =pd.to_datetime(market_train_df['time'], format='%Y-%m-%d %H:%M')\n",
    "    market_df['close_to_open'] = market_df['close'] / market_df['open']\n",
    "    market_df.drop(['time'], axis=1, inplace=True)\n",
    "    \n",
    "    news_df = news_df[columns_news]\n",
    "    news_df['sourceTimestamp']= pd.to_datetime(news_df['sourceTimestamp'], format='%Y-%m-%d %H:%M')\n",
    "    news_df['firstCreated'] = pd.to_datetime(news_df['firstCreated'], format='%Y-%m-%d %H:%M')\n",
    "    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n",
    "    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n",
    "    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n",
    "    news_df['len_audiences'] = news_train_df['audiences'].map(lambda x: len(eval(x)))\n",
    "    kcol = ['firstCreated', 'assetCodes']\n",
    "    news_df = news_df.groupby(kcol, as_index=False).mean()\n",
    "    market_df = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n",
    "                            right_on=['firstCreated', 'assetCodes'])\n",
    "    del news_df\n",
    "    market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n",
    "    market_df = market_df.drop(columns = ['firstCreated','assetCodes','assetName']).fillna(0) \n",
    "    return market_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assetCode</th>\n",
       "      <th>volume</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>returnsClosePrevRaw1</th>\n",
       "      <th>returnsOpenPrevRaw1</th>\n",
       "      <th>returnsClosePrevMktres1</th>\n",
       "      <th>returnsOpenPrevMktres1</th>\n",
       "      <th>returnsClosePrevRaw10</th>\n",
       "      <th>returnsOpenPrevRaw10</th>\n",
       "      <th>...</th>\n",
       "      <th>urgency</th>\n",
       "      <th>takeSequence</th>\n",
       "      <th>bodySize</th>\n",
       "      <th>companyCount</th>\n",
       "      <th>sentenceCount</th>\n",
       "      <th>firstMentionSentence</th>\n",
       "      <th>assetCodesLen</th>\n",
       "      <th>asset_sentiment_count</th>\n",
       "      <th>len_audiences</th>\n",
       "      <th>assetCodeT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.N</td>\n",
       "      <td>2729240</td>\n",
       "      <td>31.30</td>\n",
       "      <td>31.39</td>\n",
       "      <td>0.007403</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>-0.002344</td>\n",
       "      <td>0.010532</td>\n",
       "      <td>0.063179</td>\n",
       "      <td>0.065874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAI.N</td>\n",
       "      <td>3436803</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.26</td>\n",
       "      <td>-0.007663</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>-0.027851</td>\n",
       "      <td>0.021131</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAP.N</td>\n",
       "      <td>1701655</td>\n",
       "      <td>40.38</td>\n",
       "      <td>40.70</td>\n",
       "      <td>-0.002470</td>\n",
       "      <td>-0.009973</td>\n",
       "      <td>-0.012789</td>\n",
       "      <td>-0.008617</td>\n",
       "      <td>-0.008378</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL.O</td>\n",
       "      <td>17633150</td>\n",
       "      <td>214.01</td>\n",
       "      <td>213.50</td>\n",
       "      <td>0.015555</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>-0.001329</td>\n",
       "      <td>0.009182</td>\n",
       "      <td>0.115449</td>\n",
       "      <td>0.095602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAV.N</td>\n",
       "      <td>833228</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.047546</td>\n",
       "      <td>-0.003017</td>\n",
       "      <td>0.028141</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>0.057276</td>\n",
       "      <td>0.037677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  assetCode    volume   close    open  returnsClosePrevRaw1  \\\n",
       "0       A.N   2729240   31.30   31.39              0.007403   \n",
       "1     AAI.N   3436803    5.18    5.26             -0.007663   \n",
       "2     AAP.N   1701655   40.38   40.70             -0.002470   \n",
       "3    AAPL.O  17633150  214.01  213.50              0.015555   \n",
       "4     AAV.N    833228    6.83    6.61              0.047546   \n",
       "\n",
       "   returnsOpenPrevRaw1  returnsClosePrevMktres1  returnsOpenPrevMktres1  \\\n",
       "0             0.011276                -0.002344                0.010532   \n",
       "1             0.011538                -0.027851                0.021131   \n",
       "2            -0.009973                -0.012789               -0.008617   \n",
       "3             0.001830                -0.001329                0.009182   \n",
       "4            -0.003017                 0.028141               -0.000661   \n",
       "\n",
       "   returnsClosePrevRaw10  returnsOpenPrevRaw10  ...  urgency  takeSequence  \\\n",
       "0               0.063179              0.065874  ...      0.0           0.0   \n",
       "1               0.005825              0.005736  ...      0.0           0.0   \n",
       "2              -0.008378              0.002677  ...      0.0           0.0   \n",
       "3               0.115449              0.095602  ...      0.0           0.0   \n",
       "4               0.057276              0.037677  ...      0.0           0.0   \n",
       "\n",
       "   bodySize  companyCount sentenceCount  firstMentionSentence  assetCodesLen  \\\n",
       "0       0.0           0.0           0.0                   0.0            0.0   \n",
       "1       0.0           0.0           0.0                   0.0            0.0   \n",
       "2       0.0           0.0           0.0                   0.0            0.0   \n",
       "3       0.0           0.0           0.0                   0.0            0.0   \n",
       "4       0.0           0.0           0.0                   0.0            0.0   \n",
       "\n",
       "   asset_sentiment_count  len_audiences  assetCodeT  \n",
       "0                    0.0            0.0           0  \n",
       "1                    0.0            0.0           1  \n",
       "2                    0.0            0.0           2  \n",
       "3                    0.0            0.0           3  \n",
       "4                    0.0            0.0           4  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Merging data ...')\n",
    "market_train_df = data_prep(market_train_df, news_train_df)\n",
    "market_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8364\n",
       "0    6632\n",
       "Name: returnsOpenNextMktres10, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### making dependent variables\n",
    "market_train_df['returnsOpenNextMktres10'] = np.where(market_train_df.returnsOpenNextMktres10 <= 0,0,1)\n",
    "market_train_df.returnsOpenNextMktres10.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8364\n",
       "0    6632\n",
       "Name: returnsOpenNextMktres10, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### counting 0 and 1 \n",
    "c = market_train_df['returnsOpenNextMktres10'].value_counts()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### balancing zero and once. As we can see that ones is more than zero (oversampled), so I am trying to blance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "random.seed(10)\n",
    "a = market_train_df[market_train_df['returnsOpenNextMktres10']==1].sample(n = c[0])\n",
    "b = market_train_df[market_train_df['returnsOpenNextMktres10']==0]\n",
    "market_train_df = b.append(a,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6632\n",
       "0    6632\n",
       "Name: returnsOpenNextMktres10, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = market_train_df['returnsOpenNextMktres10'].value_counts()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### imputing missing value \n",
    "market_train_df['returnsOpenNextMktres10'] = np.where(market_train_df.returnsOpenNextMktres10.isna==True,0,market_train_df.returnsOpenNextMktres10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    13264\n",
       "Name: returnsOpenNextMktres10, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### cross verify is that missing value present or not \n",
    "market_train_df['returnsOpenNextMktres10'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "For categorical variables where no such ordinal relationship exists, the integer encoding is not enough.\n",
    "\n",
    "In fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n",
    "\n",
    "In this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n",
    "\n",
    "In the “color” variable example, there are 3 categories and therefore 3 binary variables are needed. A “1” value is placed in the binary variable for the color and “0” values for the other colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot-incoding with categorical variables \n",
    "columns = ['assetCode']\n",
    "\n",
    "one_hot1 = pd.get_dummies(market_train_df[['assetCode']])\n",
    "\n",
    "market_train_df = market_train_df.drop(columns,axis = 1)\n",
    "\n",
    "market_train_df=market_train_df.join(one_hot1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing With News Headlines Data \n",
    "\n",
    "#### Preprocessing of News headlines Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step should be cleaning the data in order to obtain better features. We will achieve this by doing some of the basic pre-processing steps on our news headline data.\n",
    "\n",
    "So, let’s get into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making copy of news_train_orig data\n",
    "news_df = news_train_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case\n",
    "The first pre-processing step which we will do is transform news headline into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    china's daqing pumps 43.41 mln tonnes of oil i...\n",
       "1            feature-in kidnapping, finesse works best\n",
       "2           press digest - wall street journal - jan 1\n",
       "3                press digest - new york times - jan 1\n",
       "4                press digest - new york times - jan 1\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convering all text data into Lower case\n",
    "news_df['headline'] = news_df['headline'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "news_df['headline'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation\n",
    "The next step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    chinas daqing pumps 4341 mln tonnes of oil in 06\n",
       "1             featurein kidnapping finesse works best\n",
       "2            press digest  wall street journal  jan 1\n",
       "3                 press digest  new york times  jan 1\n",
       "4                 press digest  new york times  jan 1\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Removing Panctuation\n",
    "news_df['headline'] = news_df['headline'].str.replace('[^\\w\\s]','')\n",
    "news_df['headline'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Stop Words\n",
    "As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    chinas daqing pumps 4341 mln tonnes oil 06\n",
       "1       featurein kidnapping finesse works best\n",
       "2        press digest wall street journal jan 1\n",
       "3             press digest new york times jan 1\n",
       "4             press digest new york times jan 1\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Removing Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "news_df['headline'] = news_df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "news_df['headline'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Numeric Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     chinas daqing pumps   mln tonnes oil  \n",
       "1    featurein kidnapping finesse works best\n",
       "2     press digest wall street journal jan  \n",
       "3          press digest new york times jan  \n",
       "4          press digest new york times jan  \n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Removing numerical Value from text\n",
    "news_df['headline'] = news_df['headline'].str.replace('\\d+', ' ')\n",
    "news_df['headline'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling correction\n",
    "\n",
    "Spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” will be treated as different words even if they are used in the same sense.\n",
    "\n",
    "To achieve this we will use the textblob library. If you are not familiar with it, you can check my previous article on ‘NLP for beginners using textblob’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spelling Correction\n",
    "from textblob import TextBlob\n",
    "news_df['headline'] = news_df['headline'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Streaming text data with help of portar Stemmer library from nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "news_df['headline'] = news_df['headline'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "Unigrams do not usually contain as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n",
    "\n",
    "So, let’s quickly extract bigrams from our news headline using the ngrams function of the textblob library.\n",
    "\n",
    "### Term frequency\n",
    "Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "\n",
    "Therefore, we can generalize term frequency as:\n",
    "\n",
    "TF = (Number of times term T appears in the particular row) / (number of terms in that row)\n",
    "\n",
    "### Inverse Document Frequency\n",
    "The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.\n",
    "\n",
    "Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n",
    "\n",
    "IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n",
    "\n",
    "### Term Frequency – Inverse Document Frequency (TF-IDF)\n",
    "TF-IDF is the multiplication of the TF and IDF which we calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calulate tfidf and making n grams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    "stop_words= 'english',ngram_range=(2,2))\n",
    "train_vect = tfidf.fit_transform(news_df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_vect1 = pd.DataFrame(train_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "Dimensionality reduction using truncated SVD (aka LSA).\n",
    "\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n",
    "\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing svd(singular value dicomposition) library \n",
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
    "# from sklearn.random_projection import GaussianRandomProjection\n",
    "# from sklearn.random_projection import SparseRandomProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define svd function and pass argument \n",
    "svd = TruncatedSVD(n_components=240, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fitting tfidf values in svd function and This transformer performs linear dimensionality reduction\n",
    "data1_svd = svd.fit_transform(train_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### making data frame because svd function return values in array \n",
    "data1_svd = pd.DataFrame(data1_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1_svd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### join text data with main market data\n",
    "market_train_df = pd.concat([market_train_df, data1_svd], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 1804)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking shape of the data\n",
    "market_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Missing value present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     16736\n",
       "False    13264\n",
       "Name: returnsOpenNextMktres10, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking missing values in data\n",
    "market_train_df['returnsOpenNextMktres10'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imputing missing values \n",
    "market_train_df['returnsOpenNextMktres10'] = np.where(market_train_df.returnsOpenNextMktres10.isna()==True,0,market_train_df.returnsOpenNextMktres10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devide data set into two part X and y(dependent and independent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 1802), (30000,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### defing X and Y \n",
    "columns = ['returnsOpenNextMktres10','date']\n",
    "X = market_train_df.drop(columns, axis=1)\n",
    "y = market_train_df['returnsOpenNextMktres10']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data set into Train And Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Spliting data set into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train1,X_test1,y_train1,y_test1 = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing Library for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import matplotlib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##XGBOOST\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "Ever since its introduction in 2014, Extreme Gradient Boosting with XGBoost has been admired as the holy grail of machine learning competitions. From predicting ad click-through rates to classifying high energy physics events, XGBoost has proved its one of the best algorithms in terms of performance and speed. I always used to XGBoost as my first algorithm of building machine learning model because it saves time and gives better accuracy and provide better solution than other machine learning algorithm and it is optimized and distributed gradient boosting library. It uses gradient boosting framework at core.\n",
    "\n",
    "XGBoost was created by Tianqi Chen, PhD Student, University of Washington. It is used for supervised ML problems. Features of XGBoost algorithm.\n",
    "\n",
    "1.Parallel Computing: It is enabled with parallel processing (using OpenMP); i.e., when you run XGBoost, by default, it would use all the cores of your laptop/machine.\n",
    "\n",
    "2.Regularization: I believe this is the biggest advantage of XGBoost. GBM has no provision for regularization. Regularization is a technique used to avoid overfitting in linear and tree-based models.\n",
    "\n",
    "3.Enabled Cross Validation: In R, we usually use external packages such as caret and mlr to obtain CV results. But, XGBoost is enabled with internal CV function (we'll see below).\n",
    "\n",
    "4.Missing Values: XGBoost is designed to handle missing values internally. The missing values are treated in such a manner that if there exists any trend in missing values, it is captured by the model.\n",
    "\n",
    "5.Flexibility: In addition to regression, classification, and ranking problems, it supports user-defined objective functions also. An objective function is used to measure the performance of the model given a certain set of parameters. Furthermore, it supports user defined evaluation metrics as well.\n",
    "\n",
    "6.Availability: Currently, it is available for programming languages such as R, Python, Java, Julia, and Scala.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "clf = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning code for prevent overfiiting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "            'silent': [False],\n",
    "            'max_depth': [5, 10],##15,20\n",
    "            'learning_rate': [0.1, 0.2, 0.3],\n",
    "            'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "            'gamma': [0, 0.25, 0.5, 1.0],\n",
    "            'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0],  \n",
    "            'n_estimators': [20]}\n",
    "    \n",
    "fit_params = {'eval_metric': 'logloss',\n",
    "                 'early_stopping_rounds': 8, ##10\n",
    "                  'eval_set': [(X_test, y_test)]}\n",
    "    \n",
    "rs_clf = RandomizedSearchCV(clf, param_grid, n_iter=10,\n",
    "                                n_jobs=1, verbose=2, cv=2,\n",
    "                                fit_params=fit_params,\n",
    "                                scoring='neg_log_loss', random_state=50) #, refit=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  training the data with the help of xgboost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:643: DeprecationWarning:\n",
      "\n",
      "\"fit_params\" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the \"fit\" method instead.\n",
      "\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] subsample=0.9, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=0.9, colsample_bylevel=0.8 \n",
      "[0]\tvalidation_0-logloss:0.637164\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.591096\n",
      "[2]\tvalidation_0-logloss:0.552585\n",
      "[3]\tvalidation_0-logloss:0.519674\n",
      "[4]\tvalidation_0-logloss:0.491904\n",
      "[5]\tvalidation_0-logloss:0.467967\n",
      "[6]\tvalidation_0-logloss:0.447061\n",
      "[7]\tvalidation_0-logloss:0.429132\n",
      "[8]\tvalidation_0-logloss:0.41306\n",
      "[9]\tvalidation_0-logloss:0.398693\n",
      "[10]\tvalidation_0-logloss:0.386282\n",
      "[11]\tvalidation_0-logloss:0.37501\n",
      "[12]\tvalidation_0-logloss:0.364995\n",
      "[13]\tvalidation_0-logloss:0.356148\n",
      "[14]\tvalidation_0-logloss:0.348034\n",
      "[15]\tvalidation_0-logloss:0.341543\n",
      "[16]\tvalidation_0-logloss:0.33558\n",
      "[17]\tvalidation_0-logloss:0.330191\n",
      "[18]\tvalidation_0-logloss:0.324972\n",
      "[19]\tvalidation_0-logloss:0.320538\n",
      "[CV]  subsample=0.9, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=0.9, colsample_bylevel=0.8, total=  50.0s\n",
      "[CV] subsample=0.9, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=0.9, colsample_bylevel=0.8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   51.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.637115\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.593241\n",
      "[2]\tvalidation_0-logloss:0.554015\n",
      "[3]\tvalidation_0-logloss:0.522128\n",
      "[4]\tvalidation_0-logloss:0.493508\n",
      "[5]\tvalidation_0-logloss:0.468819\n",
      "[6]\tvalidation_0-logloss:0.447355\n",
      "[7]\tvalidation_0-logloss:0.428482\n",
      "[8]\tvalidation_0-logloss:0.412377\n",
      "[9]\tvalidation_0-logloss:0.39831\n",
      "[10]\tvalidation_0-logloss:0.385502\n",
      "[11]\tvalidation_0-logloss:0.374562\n",
      "[12]\tvalidation_0-logloss:0.364731\n",
      "[13]\tvalidation_0-logloss:0.356299\n",
      "[14]\tvalidation_0-logloss:0.348402\n",
      "[15]\tvalidation_0-logloss:0.34157\n",
      "[16]\tvalidation_0-logloss:0.335174\n",
      "[17]\tvalidation_0-logloss:0.329552\n",
      "[18]\tvalidation_0-logloss:0.324595\n",
      "[19]\tvalidation_0-logloss:0.320213\n",
      "[CV]  subsample=0.9, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=0.9, colsample_bylevel=0.8, total=  50.0s\n",
      "[CV] subsample=0.7, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=1.0, max_depth=10, learning_rate=0.1, gamma=0, colsample_bytree=0.7, colsample_bylevel=0.7 \n",
      "[0]\tvalidation_0-logloss:0.637848\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.591584\n",
      "[2]\tvalidation_0-logloss:0.552981\n",
      "[3]\tvalidation_0-logloss:0.519855\n",
      "[4]\tvalidation_0-logloss:0.491861\n",
      "[5]\tvalidation_0-logloss:0.467572\n",
      "[6]\tvalidation_0-logloss:0.447164\n",
      "[7]\tvalidation_0-logloss:0.428927\n",
      "[8]\tvalidation_0-logloss:0.413867\n",
      "[9]\tvalidation_0-logloss:0.400016\n",
      "[10]\tvalidation_0-logloss:0.38837\n",
      "[11]\tvalidation_0-logloss:0.377892\n",
      "[12]\tvalidation_0-logloss:0.367997\n",
      "[13]\tvalidation_0-logloss:0.359276\n",
      "[14]\tvalidation_0-logloss:0.351958\n",
      "[15]\tvalidation_0-logloss:0.344979\n",
      "[16]\tvalidation_0-logloss:0.33862\n",
      "[17]\tvalidation_0-logloss:0.33272\n",
      "[18]\tvalidation_0-logloss:0.327712\n",
      "[19]\tvalidation_0-logloss:0.323425\n",
      "[CV]  subsample=0.7, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=1.0, max_depth=10, learning_rate=0.1, gamma=0, colsample_bytree=0.7, colsample_bylevel=0.7, total=  38.3s\n",
      "[CV] subsample=0.7, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=1.0, max_depth=10, learning_rate=0.1, gamma=0, colsample_bytree=0.7, colsample_bylevel=0.7 \n",
      "[0]\tvalidation_0-logloss:0.636399\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.589322\n",
      "[2]\tvalidation_0-logloss:0.550117\n",
      "[3]\tvalidation_0-logloss:0.517725\n",
      "[4]\tvalidation_0-logloss:0.489209\n",
      "[5]\tvalidation_0-logloss:0.464669\n",
      "[6]\tvalidation_0-logloss:0.444175\n",
      "[7]\tvalidation_0-logloss:0.425463\n",
      "[8]\tvalidation_0-logloss:0.409862\n",
      "[9]\tvalidation_0-logloss:0.395699\n",
      "[10]\tvalidation_0-logloss:0.383393\n",
      "[11]\tvalidation_0-logloss:0.372724\n",
      "[12]\tvalidation_0-logloss:0.362751\n",
      "[13]\tvalidation_0-logloss:0.354319\n",
      "[14]\tvalidation_0-logloss:0.347005\n",
      "[15]\tvalidation_0-logloss:0.340584\n",
      "[16]\tvalidation_0-logloss:0.334855\n",
      "[17]\tvalidation_0-logloss:0.329663\n",
      "[18]\tvalidation_0-logloss:0.32461\n",
      "[19]\tvalidation_0-logloss:0.319986\n",
      "[CV]  subsample=0.7, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=1.0, max_depth=10, learning_rate=0.1, gamma=0, colsample_bytree=0.7, colsample_bylevel=0.7, total=  39.1s\n",
      "[CV] subsample=0.6, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=0.4, colsample_bylevel=0.9 \n",
      "[0]\tvalidation_0-logloss:0.638609\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.592834\n",
      "[2]\tvalidation_0-logloss:0.55526\n",
      "[3]\tvalidation_0-logloss:0.522466\n",
      "[4]\tvalidation_0-logloss:0.495029\n",
      "[5]\tvalidation_0-logloss:0.470618\n",
      "[6]\tvalidation_0-logloss:0.449314\n",
      "[7]\tvalidation_0-logloss:0.431843\n",
      "[8]\tvalidation_0-logloss:0.416316\n",
      "[9]\tvalidation_0-logloss:0.402786\n",
      "[10]\tvalidation_0-logloss:0.390406\n",
      "[11]\tvalidation_0-logloss:0.379425\n",
      "[12]\tvalidation_0-logloss:0.36953\n",
      "[13]\tvalidation_0-logloss:0.36075\n",
      "[14]\tvalidation_0-logloss:0.353561\n",
      "[15]\tvalidation_0-logloss:0.34628\n",
      "[16]\tvalidation_0-logloss:0.340404\n",
      "[17]\tvalidation_0-logloss:0.334811\n",
      "[18]\tvalidation_0-logloss:0.330192\n",
      "[19]\tvalidation_0-logloss:0.326016\n",
      "[CV]  subsample=0.6, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=0.4, colsample_bylevel=0.9, total=  29.5s\n",
      "[CV] subsample=0.6, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=0.4, colsample_bylevel=0.9 \n",
      "[0]\tvalidation_0-logloss:0.636954\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.593315\n",
      "[2]\tvalidation_0-logloss:0.554349\n",
      "[3]\tvalidation_0-logloss:0.521198\n",
      "[4]\tvalidation_0-logloss:0.495138\n",
      "[5]\tvalidation_0-logloss:0.47171\n",
      "[6]\tvalidation_0-logloss:0.451809\n",
      "[7]\tvalidation_0-logloss:0.434575\n",
      "[8]\tvalidation_0-logloss:0.417406\n",
      "[9]\tvalidation_0-logloss:0.403142\n",
      "[10]\tvalidation_0-logloss:0.390222\n",
      "[11]\tvalidation_0-logloss:0.378476\n",
      "[12]\tvalidation_0-logloss:0.368212\n",
      "[13]\tvalidation_0-logloss:0.360321\n",
      "[14]\tvalidation_0-logloss:0.352307\n",
      "[15]\tvalidation_0-logloss:0.345052\n",
      "[16]\tvalidation_0-logloss:0.339195\n",
      "[17]\tvalidation_0-logloss:0.333727\n",
      "[18]\tvalidation_0-logloss:0.328049\n",
      "[19]\tvalidation_0-logloss:0.323713\n",
      "[CV]  subsample=0.6, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=0.4, colsample_bylevel=0.9, total=  28.7s\n",
      "[CV] subsample=1.0, silent=False, reg_lambda=50.0, n_estimators=20, min_child_weight=0.5, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=1.0, colsample_bylevel=0.9 \n",
      "[0]\tvalidation_0-logloss:0.638533\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.593625\n",
      "[2]\tvalidation_0-logloss:0.555941\n",
      "[3]\tvalidation_0-logloss:0.523858\n",
      "[4]\tvalidation_0-logloss:0.496596\n",
      "[5]\tvalidation_0-logloss:0.472773\n",
      "[6]\tvalidation_0-logloss:0.451831\n",
      "[7]\tvalidation_0-logloss:0.433708\n",
      "[8]\tvalidation_0-logloss:0.417948\n",
      "[9]\tvalidation_0-logloss:0.404089\n",
      "[10]\tvalidation_0-logloss:0.391763\n",
      "[11]\tvalidation_0-logloss:0.380776\n",
      "[12]\tvalidation_0-logloss:0.370895\n",
      "[13]\tvalidation_0-logloss:0.362316\n",
      "[14]\tvalidation_0-logloss:0.354491\n",
      "[15]\tvalidation_0-logloss:0.347748\n",
      "[16]\tvalidation_0-logloss:0.34141\n",
      "[17]\tvalidation_0-logloss:0.335771\n",
      "[18]\tvalidation_0-logloss:0.330963\n",
      "[19]\tvalidation_0-logloss:0.326521\n",
      "[CV]  subsample=1.0, silent=False, reg_lambda=50.0, n_estimators=20, min_child_weight=0.5, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=1.0, colsample_bylevel=0.9, total=  57.9s\n",
      "[CV] subsample=1.0, silent=False, reg_lambda=50.0, n_estimators=20, min_child_weight=0.5, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=1.0, colsample_bylevel=0.9 \n",
      "[0]\tvalidation_0-logloss:0.638077\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.592593\n",
      "[2]\tvalidation_0-logloss:0.554797\n",
      "[3]\tvalidation_0-logloss:0.522812\n",
      "[4]\tvalidation_0-logloss:0.494928\n",
      "[5]\tvalidation_0-logloss:0.470908\n",
      "[6]\tvalidation_0-logloss:0.449922\n",
      "[7]\tvalidation_0-logloss:0.431916\n",
      "[8]\tvalidation_0-logloss:0.415561\n",
      "[9]\tvalidation_0-logloss:0.401327\n",
      "[10]\tvalidation_0-logloss:0.388811\n",
      "[11]\tvalidation_0-logloss:0.378066\n",
      "[12]\tvalidation_0-logloss:0.368352\n",
      "[13]\tvalidation_0-logloss:0.359785\n",
      "[14]\tvalidation_0-logloss:0.351853\n",
      "[15]\tvalidation_0-logloss:0.344753\n",
      "[16]\tvalidation_0-logloss:0.33862\n",
      "[17]\tvalidation_0-logloss:0.332901\n",
      "[18]\tvalidation_0-logloss:0.327786\n",
      "[19]\tvalidation_0-logloss:0.323207\n",
      "[CV]  subsample=1.0, silent=False, reg_lambda=50.0, n_estimators=20, min_child_weight=0.5, max_depth=10, learning_rate=0.1, gamma=0.5, colsample_bytree=1.0, colsample_bylevel=0.9, total=  56.8s\n",
      "[CV] subsample=0.5, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=7.0, max_depth=10, learning_rate=0.3, gamma=0.5, colsample_bytree=1.0, colsample_bylevel=0.5 \n",
      "[0]\tvalidation_0-logloss:0.545362\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.462596\n",
      "[2]\tvalidation_0-logloss:0.409667\n",
      "[3]\tvalidation_0-logloss:0.374738\n",
      "[4]\tvalidation_0-logloss:0.35035\n",
      "[5]\tvalidation_0-logloss:0.332629\n",
      "[6]\tvalidation_0-logloss:0.320193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\tvalidation_0-logloss:0.312141\n",
      "[8]\tvalidation_0-logloss:0.305932\n",
      "[9]\tvalidation_0-logloss:0.301646\n",
      "[10]\tvalidation_0-logloss:0.298827\n",
      "[11]\tvalidation_0-logloss:0.297614\n",
      "[12]\tvalidation_0-logloss:0.295884\n",
      "[13]\tvalidation_0-logloss:0.295063\n",
      "[14]\tvalidation_0-logloss:0.29439\n",
      "[15]\tvalidation_0-logloss:0.294269\n",
      "[16]\tvalidation_0-logloss:0.294961\n",
      "[17]\tvalidation_0-logloss:0.294894\n",
      "[18]\tvalidation_0-logloss:0.29528\n",
      "[19]\tvalidation_0-logloss:0.295068\n",
      "[CV]  subsample=0.5, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=7.0, max_depth=10, learning_rate=0.3, gamma=0.5, colsample_bytree=1.0, colsample_bylevel=0.5, total=  35.2s\n",
      "[CV] subsample=0.5, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=7.0, max_depth=10, learning_rate=0.3, gamma=0.5, colsample_bytree=1.0, colsample_bylevel=0.5 \n",
      "[0]\tvalidation_0-logloss:0.542159\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.45945\n",
      "[2]\tvalidation_0-logloss:0.407861\n",
      "[3]\tvalidation_0-logloss:0.373254\n",
      "[4]\tvalidation_0-logloss:0.348484\n",
      "[5]\tvalidation_0-logloss:0.331788\n",
      "[6]\tvalidation_0-logloss:0.319932\n",
      "[7]\tvalidation_0-logloss:0.310883\n",
      "[8]\tvalidation_0-logloss:0.304606\n",
      "[9]\tvalidation_0-logloss:0.300178\n",
      "[10]\tvalidation_0-logloss:0.297141\n",
      "[11]\tvalidation_0-logloss:0.294047\n",
      "[12]\tvalidation_0-logloss:0.292315\n",
      "[13]\tvalidation_0-logloss:0.291526\n",
      "[14]\tvalidation_0-logloss:0.29036\n",
      "[15]\tvalidation_0-logloss:0.289512\n",
      "[16]\tvalidation_0-logloss:0.28968\n",
      "[17]\tvalidation_0-logloss:0.28887\n",
      "[18]\tvalidation_0-logloss:0.289928\n",
      "[19]\tvalidation_0-logloss:0.290126\n",
      "[CV]  subsample=0.5, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=7.0, max_depth=10, learning_rate=0.3, gamma=0.5, colsample_bytree=1.0, colsample_bylevel=0.5, total=  36.0s\n",
      "[CV] subsample=0.5, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=3.0, max_depth=10, learning_rate=0.3, gamma=0.25, colsample_bytree=0.4, colsample_bylevel=0.8 \n",
      "[0]\tvalidation_0-logloss:0.547694\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.465169\n",
      "[2]\tvalidation_0-logloss:0.412804\n",
      "[3]\tvalidation_0-logloss:0.376852\n",
      "[4]\tvalidation_0-logloss:0.352835\n",
      "[5]\tvalidation_0-logloss:0.335474\n",
      "[6]\tvalidation_0-logloss:0.324339\n",
      "[7]\tvalidation_0-logloss:0.317228\n",
      "[8]\tvalidation_0-logloss:0.311987\n",
      "[9]\tvalidation_0-logloss:0.307177\n",
      "[10]\tvalidation_0-logloss:0.304201\n",
      "[11]\tvalidation_0-logloss:0.301712\n",
      "[12]\tvalidation_0-logloss:0.299015\n",
      "[13]\tvalidation_0-logloss:0.297938\n",
      "[14]\tvalidation_0-logloss:0.29812\n",
      "[15]\tvalidation_0-logloss:0.297377\n",
      "[16]\tvalidation_0-logloss:0.29753\n",
      "[17]\tvalidation_0-logloss:0.297224\n",
      "[18]\tvalidation_0-logloss:0.298248\n",
      "[19]\tvalidation_0-logloss:0.299158\n",
      "[CV]  subsample=0.5, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=3.0, max_depth=10, learning_rate=0.3, gamma=0.25, colsample_bytree=0.4, colsample_bylevel=0.8, total=  27.8s\n",
      "[CV] subsample=0.5, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=3.0, max_depth=10, learning_rate=0.3, gamma=0.25, colsample_bytree=0.4, colsample_bylevel=0.8 \n",
      "[0]\tvalidation_0-logloss:0.540281\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.461164\n",
      "[2]\tvalidation_0-logloss:0.408358\n",
      "[3]\tvalidation_0-logloss:0.372049\n",
      "[4]\tvalidation_0-logloss:0.349024\n",
      "[5]\tvalidation_0-logloss:0.331732\n",
      "[6]\tvalidation_0-logloss:0.320697\n",
      "[7]\tvalidation_0-logloss:0.312661\n",
      "[8]\tvalidation_0-logloss:0.305006\n",
      "[9]\tvalidation_0-logloss:0.300995\n",
      "[10]\tvalidation_0-logloss:0.29836\n",
      "[11]\tvalidation_0-logloss:0.295914\n",
      "[12]\tvalidation_0-logloss:0.294569\n",
      "[13]\tvalidation_0-logloss:0.294387\n",
      "[14]\tvalidation_0-logloss:0.293262\n",
      "[15]\tvalidation_0-logloss:0.292739\n",
      "[16]\tvalidation_0-logloss:0.292527\n",
      "[17]\tvalidation_0-logloss:0.292419\n",
      "[18]\tvalidation_0-logloss:0.292241\n",
      "[19]\tvalidation_0-logloss:0.292309\n",
      "[CV]  subsample=0.5, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=3.0, max_depth=10, learning_rate=0.3, gamma=0.25, colsample_bytree=0.4, colsample_bylevel=0.8, total=  29.0s\n",
      "[CV] subsample=0.9, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=5.0, max_depth=10, learning_rate=0.1, gamma=1.0, colsample_bytree=0.9, colsample_bylevel=0.5 \n",
      "[0]\tvalidation_0-logloss:0.636652\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.590436\n",
      "[2]\tvalidation_0-logloss:0.551687\n",
      "[3]\tvalidation_0-logloss:0.519071\n",
      "[4]\tvalidation_0-logloss:0.49136\n",
      "[5]\tvalidation_0-logloss:0.467107\n",
      "[6]\tvalidation_0-logloss:0.44631\n",
      "[7]\tvalidation_0-logloss:0.428241\n",
      "[8]\tvalidation_0-logloss:0.412527\n",
      "[9]\tvalidation_0-logloss:0.398249\n",
      "[10]\tvalidation_0-logloss:0.385895\n",
      "[11]\tvalidation_0-logloss:0.374574\n",
      "[12]\tvalidation_0-logloss:0.364471\n",
      "[13]\tvalidation_0-logloss:0.356258\n",
      "[14]\tvalidation_0-logloss:0.3485\n",
      "[15]\tvalidation_0-logloss:0.341444\n",
      "[16]\tvalidation_0-logloss:0.335406\n",
      "[17]\tvalidation_0-logloss:0.330197\n",
      "[18]\tvalidation_0-logloss:0.325377\n",
      "[19]\tvalidation_0-logloss:0.320984\n",
      "[CV]  subsample=0.9, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=5.0, max_depth=10, learning_rate=0.1, gamma=1.0, colsample_bytree=0.9, colsample_bylevel=0.5, total=  35.7s\n",
      "[CV] subsample=0.9, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=5.0, max_depth=10, learning_rate=0.1, gamma=1.0, colsample_bytree=0.9, colsample_bylevel=0.5 \n",
      "[0]\tvalidation_0-logloss:0.637054\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.59303\n",
      "[2]\tvalidation_0-logloss:0.553344\n",
      "[3]\tvalidation_0-logloss:0.520872\n",
      "[4]\tvalidation_0-logloss:0.492982\n",
      "[5]\tvalidation_0-logloss:0.467838\n",
      "[6]\tvalidation_0-logloss:0.446804\n",
      "[7]\tvalidation_0-logloss:0.428297\n",
      "[8]\tvalidation_0-logloss:0.412146\n",
      "[9]\tvalidation_0-logloss:0.397392\n",
      "[10]\tvalidation_0-logloss:0.385244\n",
      "[11]\tvalidation_0-logloss:0.373798\n",
      "[12]\tvalidation_0-logloss:0.363735\n",
      "[13]\tvalidation_0-logloss:0.355077\n",
      "[14]\tvalidation_0-logloss:0.347422\n",
      "[15]\tvalidation_0-logloss:0.340208\n",
      "[16]\tvalidation_0-logloss:0.333995\n",
      "[17]\tvalidation_0-logloss:0.328464\n",
      "[18]\tvalidation_0-logloss:0.323385\n",
      "[19]\tvalidation_0-logloss:0.319263\n",
      "[CV]  subsample=0.9, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=5.0, max_depth=10, learning_rate=0.1, gamma=1.0, colsample_bytree=0.9, colsample_bylevel=0.5, total=  35.7s\n",
      "[CV] subsample=0.9, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=0.5, max_depth=5, learning_rate=0.2, gamma=1.0, colsample_bytree=0.8, colsample_bylevel=0.9 \n",
      "[0]\tvalidation_0-logloss:0.586659\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.514652\n",
      "[2]\tvalidation_0-logloss:0.463043\n",
      "[3]\tvalidation_0-logloss:0.424446\n",
      "[4]\tvalidation_0-logloss:0.39452\n",
      "[5]\tvalidation_0-logloss:0.37157\n",
      "[6]\tvalidation_0-logloss:0.353544\n",
      "[7]\tvalidation_0-logloss:0.339335\n",
      "[8]\tvalidation_0-logloss:0.327773\n",
      "[9]\tvalidation_0-logloss:0.319059\n",
      "[10]\tvalidation_0-logloss:0.311664\n",
      "[11]\tvalidation_0-logloss:0.305779\n",
      "[12]\tvalidation_0-logloss:0.300243\n",
      "[13]\tvalidation_0-logloss:0.296528\n",
      "[14]\tvalidation_0-logloss:0.293542\n",
      "[15]\tvalidation_0-logloss:0.290817\n",
      "[16]\tvalidation_0-logloss:0.288275\n",
      "[17]\tvalidation_0-logloss:0.286317\n",
      "[18]\tvalidation_0-logloss:0.28505\n",
      "[19]\tvalidation_0-logloss:0.283839\n",
      "[CV]  subsample=0.9, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=0.5, max_depth=5, learning_rate=0.2, gamma=1.0, colsample_bytree=0.8, colsample_bylevel=0.9, total=  29.6s\n",
      "[CV] subsample=0.9, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=0.5, max_depth=5, learning_rate=0.2, gamma=1.0, colsample_bytree=0.8, colsample_bylevel=0.9 \n",
      "[0]\tvalidation_0-logloss:0.586375\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.51359\n",
      "[2]\tvalidation_0-logloss:0.461037\n",
      "[3]\tvalidation_0-logloss:0.422603\n",
      "[4]\tvalidation_0-logloss:0.393232\n",
      "[5]\tvalidation_0-logloss:0.369859\n",
      "[6]\tvalidation_0-logloss:0.352091\n",
      "[7]\tvalidation_0-logloss:0.337485\n",
      "[8]\tvalidation_0-logloss:0.326033\n",
      "[9]\tvalidation_0-logloss:0.317165\n",
      "[10]\tvalidation_0-logloss:0.310448\n",
      "[11]\tvalidation_0-logloss:0.304473\n",
      "[12]\tvalidation_0-logloss:0.29904\n",
      "[13]\tvalidation_0-logloss:0.294981\n",
      "[14]\tvalidation_0-logloss:0.291467\n",
      "[15]\tvalidation_0-logloss:0.28881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16]\tvalidation_0-logloss:0.286856\n",
      "[17]\tvalidation_0-logloss:0.284937\n",
      "[18]\tvalidation_0-logloss:0.283888\n",
      "[19]\tvalidation_0-logloss:0.28273\n",
      "[CV]  subsample=0.9, silent=False, reg_lambda=10.0, n_estimators=20, min_child_weight=0.5, max_depth=5, learning_rate=0.2, gamma=1.0, colsample_bytree=0.8, colsample_bylevel=0.9, total=  29.7s\n",
      "[CV] subsample=0.6, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.2, gamma=0, colsample_bytree=0.5, colsample_bylevel=1.0 \n",
      "[0]\tvalidation_0-logloss:0.588783\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.522234\n",
      "[2]\tvalidation_0-logloss:0.469519\n",
      "[3]\tvalidation_0-logloss:0.433066\n",
      "[4]\tvalidation_0-logloss:0.403584\n",
      "[5]\tvalidation_0-logloss:0.382888\n",
      "[6]\tvalidation_0-logloss:0.364828\n",
      "[7]\tvalidation_0-logloss:0.350381\n",
      "[8]\tvalidation_0-logloss:0.3376\n",
      "[9]\tvalidation_0-logloss:0.329223\n",
      "[10]\tvalidation_0-logloss:0.3219\n",
      "[11]\tvalidation_0-logloss:0.315225\n",
      "[12]\tvalidation_0-logloss:0.309587\n",
      "[13]\tvalidation_0-logloss:0.305275\n",
      "[14]\tvalidation_0-logloss:0.30262\n",
      "[15]\tvalidation_0-logloss:0.300608\n",
      "[16]\tvalidation_0-logloss:0.298728\n",
      "[17]\tvalidation_0-logloss:0.29637\n",
      "[18]\tvalidation_0-logloss:0.294628\n",
      "[19]\tvalidation_0-logloss:0.293856\n",
      "[CV]  subsample=0.6, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.2, gamma=0, colsample_bytree=0.5, colsample_bylevel=1.0, total=  41.9s\n",
      "[CV] subsample=0.6, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.2, gamma=0, colsample_bytree=0.5, colsample_bylevel=1.0 \n",
      "[0]\tvalidation_0-logloss:0.585852\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.515301\n",
      "[2]\tvalidation_0-logloss:0.463969\n",
      "[3]\tvalidation_0-logloss:0.426785\n",
      "[4]\tvalidation_0-logloss:0.397573\n",
      "[5]\tvalidation_0-logloss:0.37459\n",
      "[6]\tvalidation_0-logloss:0.357215\n",
      "[7]\tvalidation_0-logloss:0.343604\n",
      "[8]\tvalidation_0-logloss:0.33259\n",
      "[9]\tvalidation_0-logloss:0.323286\n",
      "[10]\tvalidation_0-logloss:0.316366\n",
      "[11]\tvalidation_0-logloss:0.310464\n",
      "[12]\tvalidation_0-logloss:0.305602\n",
      "[13]\tvalidation_0-logloss:0.301857\n",
      "[14]\tvalidation_0-logloss:0.298399\n",
      "[15]\tvalidation_0-logloss:0.295123\n",
      "[16]\tvalidation_0-logloss:0.293099\n",
      "[17]\tvalidation_0-logloss:0.291456\n",
      "[18]\tvalidation_0-logloss:0.29015\n",
      "[19]\tvalidation_0-logloss:0.289141\n",
      "[CV]  subsample=0.6, silent=False, reg_lambda=5.0, n_estimators=20, min_child_weight=10.0, max_depth=10, learning_rate=0.2, gamma=0, colsample_bytree=0.5, colsample_bylevel=1.0, total=  42.0s\n",
      "[CV] subsample=0.5, silent=False, reg_lambda=1.0, n_estimators=20, min_child_weight=3.0, max_depth=5, learning_rate=0.1, gamma=0.25, colsample_bytree=0.4, colsample_bylevel=0.8 \n",
      "[0]\tvalidation_0-logloss:0.637891\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.593902\n",
      "[2]\tvalidation_0-logloss:0.555609\n",
      "[3]\tvalidation_0-logloss:0.523072\n",
      "[4]\tvalidation_0-logloss:0.496293\n",
      "[5]\tvalidation_0-logloss:0.47119\n",
      "[6]\tvalidation_0-logloss:0.45128\n",
      "[7]\tvalidation_0-logloss:0.433343\n",
      "[8]\tvalidation_0-logloss:0.417965\n",
      "[9]\tvalidation_0-logloss:0.404479\n",
      "[10]\tvalidation_0-logloss:0.391184\n",
      "[11]\tvalidation_0-logloss:0.380202\n",
      "[12]\tvalidation_0-logloss:0.370875\n",
      "[13]\tvalidation_0-logloss:0.361072\n",
      "[14]\tvalidation_0-logloss:0.353696\n",
      "[15]\tvalidation_0-logloss:0.346713\n",
      "[16]\tvalidation_0-logloss:0.33936\n",
      "[17]\tvalidation_0-logloss:0.333358\n",
      "[18]\tvalidation_0-logloss:0.328555\n",
      "[19]\tvalidation_0-logloss:0.324154\n",
      "[CV]  subsample=0.5, silent=False, reg_lambda=1.0, n_estimators=20, min_child_weight=3.0, max_depth=5, learning_rate=0.1, gamma=0.25, colsample_bytree=0.4, colsample_bylevel=0.8, total=  21.4s\n",
      "[CV] subsample=0.5, silent=False, reg_lambda=1.0, n_estimators=20, min_child_weight=3.0, max_depth=5, learning_rate=0.1, gamma=0.25, colsample_bytree=0.4, colsample_bylevel=0.8 \n",
      "[0]\tvalidation_0-logloss:0.635809\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.591975\n",
      "[2]\tvalidation_0-logloss:0.551941\n",
      "[3]\tvalidation_0-logloss:0.518527\n",
      "[4]\tvalidation_0-logloss:0.491141\n",
      "[5]\tvalidation_0-logloss:0.46689\n",
      "[6]\tvalidation_0-logloss:0.446825\n",
      "[7]\tvalidation_0-logloss:0.428841\n",
      "[8]\tvalidation_0-logloss:0.413492\n",
      "[9]\tvalidation_0-logloss:0.400349\n",
      "[10]\tvalidation_0-logloss:0.388285\n",
      "[11]\tvalidation_0-logloss:0.377831\n",
      "[12]\tvalidation_0-logloss:0.367627\n",
      "[13]\tvalidation_0-logloss:0.358454\n",
      "[14]\tvalidation_0-logloss:0.350478\n",
      "[15]\tvalidation_0-logloss:0.34326\n",
      "[16]\tvalidation_0-logloss:0.337251\n",
      "[17]\tvalidation_0-logloss:0.331124\n",
      "[18]\tvalidation_0-logloss:0.326164\n",
      "[19]\tvalidation_0-logloss:0.321427\n",
      "[CV]  subsample=0.5, silent=False, reg_lambda=1.0, n_estimators=20, min_child_weight=3.0, max_depth=5, learning_rate=0.1, gamma=0.25, colsample_bytree=0.4, colsample_bylevel=0.8, total=  20.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.586192\n",
      "Will train until validation_0-logloss hasn't improved in 8 rounds.\n",
      "[1]\tvalidation_0-logloss:0.513609\n",
      "[2]\tvalidation_0-logloss:0.461814\n",
      "[3]\tvalidation_0-logloss:0.423407\n",
      "[4]\tvalidation_0-logloss:0.393382\n",
      "[5]\tvalidation_0-logloss:0.370723\n",
      "[6]\tvalidation_0-logloss:0.352658\n",
      "[7]\tvalidation_0-logloss:0.338085\n",
      "[8]\tvalidation_0-logloss:0.326855\n",
      "[9]\tvalidation_0-logloss:0.317892\n",
      "[10]\tvalidation_0-logloss:0.310224\n",
      "[11]\tvalidation_0-logloss:0.304052\n",
      "[12]\tvalidation_0-logloss:0.299217\n",
      "[13]\tvalidation_0-logloss:0.295189\n",
      "[14]\tvalidation_0-logloss:0.291533\n",
      "[15]\tvalidation_0-logloss:0.288895\n",
      "[16]\tvalidation_0-logloss:0.286756\n",
      "[17]\tvalidation_0-logloss:0.284952\n",
      "[18]\tvalidation_0-logloss:0.283178\n",
      "[19]\tvalidation_0-logloss:0.281957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2, error_score='raise-deprecating',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "          fit_params={'eval_metric': 'logloss', 'early_stopping_rounds': 8, 'eval_set': [(           volume  close    open  returnsClosePrevRaw1  returnsOpenPrevRaw1  \\\n",
       "18811         NaN    NaN     NaN                   NaN                  NaN\n",
       "26023         NaN    NaN     NaN                   NaN        ...015     0.0\n",
       "23061    0.0\n",
       "24750    0.0\n",
       "Name: returnsOpenNextMktres10, Length: 6000, dtype: float64)]},\n",
       "          iid='warn', n_iter=10, n_jobs=1,\n",
       "          param_distributions={'silent': [False], 'max_depth': [5, 10], 'learning_rate': [0.1, 0.2, 0.3], 'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0], 'gamma': [0, 0.25, 0.5, 1.0], 'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0], 'n_estimators': [20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=50, refit=True,\n",
       "          return_train_score='warn', scoring='neg_log_loss', verbose=2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defing function for calculting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sensitivity = 1-specificity(roc curve, where sensitivity and specificity cut at same point = optimal threshold)\n",
    "## The optimal cut off point would be where true positive rate is high and the false positive rate is low.\n",
    "##Based on this logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve: \n",
    "This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr))\n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n",
    "    return list(roc_t['threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Probability on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47086924, 0.43957728, 0.53854614, ..., 0.00905202, 0.39770377,\n",
       "       0.00905202], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_train_pred = rs_clf.predict_proba(X_train)[:,1]\n",
    "nn_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43181565403938293]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning:\n",
      "\n",
      "\n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "threshold = Find_Optimal_Cutoff(y_train, nn_train_pred)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15987,  2688],\n",
       "       [  765,  4560]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "nn_train_pred1 = np.where(nn_train_pred >=threshold,1,0)\n",
    "cm1=confusion_matrix(y_train, nn_train_pred1)\n",
    "cm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity, Specificity and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "            Predicted No    Predicted Yes\n",
    "            \n",
    "Actual No    TN= 15987                 FP=2688     Total of Actual No = 18675\n",
    "\n",
    "Actual yes   FN= 765                   TP=4560     Total of Actual yes = 5325\n",
    "            Total= 16752               Total= 7248\n",
    "            \n",
    "            \n",
    "### Sensitivity or Recall\n",
    "TPR - When it actual yes and how often does predict yes\n",
    "TP/actual yes = 4560/765+4560= 0.856\n",
    "### Specificity\n",
    "TNR - When it actually no how often does it predict no\n",
    "TN/actual no = 15987/15987+2688 = 0.856\n",
    "\n",
    "### Accuracy\n",
    "Overall, how often is the classifier correct..\n",
    "(TP+TN)/Total = (15987+4560)/24000 = 0.856\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Sensitivity, Specificity andn Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856338028169014\n",
      "0.8560642570281124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.856125"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "sensitivity= cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "accuracy_score(y_train, nn_train_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Probability on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00905202, 0.00905202, 0.23425336, ..., 0.29610324, 0.00905202,\n",
       "       0.00905202], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_test_pred =  rs_clf.predict_proba(X_test)[:,1]\n",
    "nn_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3961,  732],\n",
       "       [ 212, 1095]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_test_pred1 = np.where(nn_test_pred >=threshold,1,0)\n",
    "cm2=confusion_matrix(y_test, nn_test_pred1)\n",
    "\n",
    "cm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating Sensitivity Specificity and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837796480489671\n",
      "0.8440230129980822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8426666666666667"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = cm2[0,0]/(cm2[0,0]+cm2[0,1])\n",
    "sensitivity= cm2[1,1]/(cm2[1,0]+cm2[1,1])\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "accuracy_score(y_test, nn_test_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up decay learning rate\n",
    "def learning_rate_power(current_round):\n",
    "    base_learning_rate = 0.19000424246380565\n",
    "    min_learning_rate = 0.01\n",
    "    lr = base_learning_rate * np.power(0.995,current_round)\n",
    "    return max(lr, min_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "# tune_params = {'n_estimators': [200,500,1000,2500,5000],\n",
    "#               'max_depth': sp_randint(4,12),\n",
    "#                'learning_rate' : 0.005,\n",
    "#                'bagging_fraction' : 0.7,\n",
    "#                'feature_fraction' : 0.5,\n",
    "#                'bagging_frequency' : 6,\n",
    "#                'bagging_seed' : 42,\n",
    "#               'colsample_bytree':sp_uniform(loc=0.8, scale=0.15),\n",
    "#               'min_child_samples':sp_randint(60,120),\n",
    "#               'subsample': sp_uniform(loc=0.75, scale=0.25),\n",
    "#               'reg_lambda':[1e-3, 1e-2, 1e-1, 1]}\n",
    "tune_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss', 'auc'},\n",
    "    'metric_freq': 1,\n",
    "    'is_training_metric': True,\n",
    "    'max_bin': 255,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 63,\n",
    "    'tree_learner': 'serial',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_sum_hessian_in_leaf': 5,\n",
    "    'is_enable_sparse': True,\n",
    "    'use_two_round_loading': False,\n",
    "    'is_save_binary_file': False,\n",
    "    'output_model': 'LightGBM_model.txt',\n",
    "    'num_machines': 1,\n",
    "    'local_listen_port': 12400,\n",
    "    'machine_list_file': 'mlist.txt',\n",
    "    'verbose': 0,\n",
    "    # parameters to keep the exactly the same\n",
    "    'subsample_for_bin': 200000,\n",
    "    'min_child_samples': 20,\n",
    "    'min_child_weight': 0.001,\n",
    "    'min_split_gain': 0.0,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'reg_alpha': 0.0,\n",
    "    'reg_lambda': 0.0\n",
    "}\n",
    "\n",
    "fit_params = {'early_stopping_rounds':40,\n",
    "              'eval_metric': 'accuracy',\n",
    "              'eval_set': [(X_train, y_train), (X_test, y_test)],\n",
    "              'verbose': 20,\n",
    "              'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_power)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(n_jobs=4, objective='binary',random_state=1)\n",
    "gs = RandomizedSearchCV(estimator=lgb_clf, \n",
    "                        param_distributions=tune_params, \n",
    "                        n_iter=10,\n",
    "                        scoring='f1',\n",
    "                        cv=5,\n",
    "                        refit=True,\n",
    "                        random_state=1,\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\lightgbm\\callback.py:189: UserWarning:\n",
      "\n",
      "Early stopping is not available in dart mode\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\ttraining's binary_logloss: 0.579053\tvalid_1's binary_logloss: 0.581023\n",
      "[40]\ttraining's binary_logloss: 0.589611\tvalid_1's binary_logloss: 0.591549\n",
      "[60]\ttraining's binary_logloss: 0.595046\tvalid_1's binary_logloss: 0.596921\n",
      "[80]\ttraining's binary_logloss: 0.593917\tvalid_1's binary_logloss: 0.595834\n",
      "[100]\ttraining's binary_logloss: 0.58821\tvalid_1's binary_logloss: 0.590276\n",
      "[120]\ttraining's binary_logloss: 0.580101\tvalid_1's binary_logloss: 0.582348\n",
      "[140]\ttraining's binary_logloss: 0.570942\tvalid_1's binary_logloss: 0.573409\n",
      "[160]\ttraining's binary_logloss: 0.561592\tvalid_1's binary_logloss: 0.564298\n",
      "[180]\ttraining's binary_logloss: 0.552528\tvalid_1's binary_logloss: 0.555491\n",
      "[200]\ttraining's binary_logloss: 0.544012\tvalid_1's binary_logloss: 0.547225\n",
      "[220]\ttraining's binary_logloss: 0.536165\tvalid_1's binary_logloss: 0.539624\n",
      "[240]\ttraining's binary_logloss: 0.529034\tvalid_1's binary_logloss: 0.53272\n",
      "[260]\ttraining's binary_logloss: 0.522595\tvalid_1's binary_logloss: 0.526478\n",
      "[280]\ttraining's binary_logloss: 0.51681\tvalid_1's binary_logloss: 0.520872\n",
      "[300]\ttraining's binary_logloss: 0.511617\tvalid_1's binary_logloss: 0.515879\n",
      "[320]\ttraining's binary_logloss: 0.506971\tvalid_1's binary_logloss: 0.511392\n",
      "[340]\ttraining's binary_logloss: 0.502821\tvalid_1's binary_logloss: 0.507372\n",
      "[360]\ttraining's binary_logloss: 0.499117\tvalid_1's binary_logloss: 0.503782\n",
      "[380]\ttraining's binary_logloss: 0.495807\tvalid_1's binary_logloss: 0.500579\n",
      "[400]\ttraining's binary_logloss: 0.492848\tvalid_1's binary_logloss: 0.497724\n",
      "[420]\ttraining's binary_logloss: 0.490202\tvalid_1's binary_logloss: 0.495173\n",
      "[440]\ttraining's binary_logloss: 0.487834\tvalid_1's binary_logloss: 0.492886\n",
      "[460]\ttraining's binary_logloss: 0.485714\tvalid_1's binary_logloss: 0.490838\n",
      "[480]\ttraining's binary_logloss: 0.483811\tvalid_1's binary_logloss: 0.489004\n",
      "[500]\ttraining's binary_logloss: 0.482106\tvalid_1's binary_logloss: 0.487359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "        min_child_samples=212, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=500, n_jobs=4, num_leaves=2452, objective='binary',\n",
       "        random_state=100, reg_alpha=0.0, reg_lambda=0.01, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(n_jobs=4,\n",
    "                             objective='multiclass',\n",
    "                            random_state=100)\n",
    "opt_params = {'n_estimators':500,\n",
    "              'boosting_type': 'dart',\n",
    "              'objective': 'binary',\n",
    "              'num_leaves':2452,\n",
    "              'min_child_samples':212,\n",
    "              'reg_lambda':0.01}\n",
    "lgb_clf.set_params(**opt_params)\n",
    "lgb_clf.fit(X_train, y_train,**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.8557083333333333\n",
      "Test accuracy:  0.8426666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Training accuracy: ', accuracy_score(y_train, lgb_clf.predict(X_train)))\n",
    "print('Test accuracy: ', accuracy_score(y_test, lgb_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48797667, 0.49387833, 0.52861003, ..., 0.29747057, 0.44331762,\n",
       "       0.29747056])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_train_pred = lgb_clf.predict_proba(X_train)[:,1]\n",
    "nn_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46296081481892276]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fakhre\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning:\n",
      "\n",
      "\n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find optimal probability threshold\n",
    "    threshold = Find_Optimal_Cutoff(y_train, nn_train_pred)\n",
    "    print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15838,  2837],\n",
       "       [  809,  4516]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "nn_train_pred1 = np.where(nn_train_pred >=threshold,1,0)\n",
    "cm1=confusion_matrix(y_train, nn_train_pred1)\n",
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848075117370892\n",
      "0.8480856760374833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8480833333333333"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "sensitivity= cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "accuracy_score(y_train, nn_train_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29802105, 0.29747057, 0.42032811, ..., 0.40774574, 0.29747056,\n",
       "       0.29747056])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_test_pred =  lgb_clf.predict_proba(X_test)[:,1]\n",
    "nn_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3936,  757],\n",
       "       [ 225, 1082]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_test_pred1 = np.where(nn_test_pred >=threshold,1,0)\n",
    "cm2=confusion_matrix(y_test, nn_test_pred1)\n",
    "\n",
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827850038255547\n",
      "0.8386959301086725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8363333333333334"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = cm2[0,0]/(cm2[0,0]+cm2[0,1])\n",
    "sensitivity= cm2[1,1]/(cm2[1,0]+cm2[1,1])\n",
    "print(sensitivity)\n",
    "print(specificity)\n",
    "accuracy_score(y_test, nn_test_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
